# -*- coding: utf-8 -*-
"""CNN Training Pipelined.ipynb

Automatically generated by Colaboratory.
Author: Rahul R
Original file is located at
    https://colab.research.google.com/drive/1CHf-Y4GWw06Bh-7aiA-cC9aUDXSmka6o
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import os
import pprint
# %tensorflow_version 1.x
import tensorflow as tf

"""## Function to Load Data"""

def create_train_data():
      j = 0 ;
      training_data=[]

      for modulation in CATEGORIES:
          path = os.path.join(DataDir,modulation)
          class_num = CATEGORIES.index(modulation)
          i = 0
          for snr in sorted(os.listdir(path)):
              try:
                  if(i > len(SNR)):
                    i = 0
                  filename = modulation+"_"+snr+".mat"
                  print(os.path.join(path,snr),SNR[i])
                  datum=loadmat(os.path.join(path,snr))
                  a = np.array(datum['receivedSignalG'])
                  training_data.append([a, class_num,SNR[i]])
                  i = i+1
                  j=j+1
                
              except Exception as e:
                  print(e)
                  j = -1
                  pass
      print("No of rows : {}".format(j))
      return training_data

def loadData():
    
  training_data = create_train_data()
  X = []
  y = []
  snr_values = []

  for features, _,snr in training_data:
      totalSamples = len(features)
      X.append(features)
      snr_values.append(snr)

  print(totalSamples,len(X))   
  X = np.array(X).reshape(-1,totalSamples)

  snr_values = np.array(snr_values).reshape(-1,1)

  print("Features shape {}:".format(np.shape(X)))
  print("SNR values shape : {}".format(np.shape(snr_values)))

  return (X,snr_values)

"""## Function to Preprocess Data For Model Training"""

def prepareData(X = 0,SNR = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],snr_values = 0,totalSamples = 500000,sampleWindowSize = 1000,splitPercent = 0.1,save = 1,CATEGORIES = ["BPSK","16_PSK","4_QAM","16_QAM","8_PSK","64_QAM"]): 

  serial_no = sampleWindowSize
  modelname = 'CNN'+str(serial_no)
  filepath = "/content/drive/My Drive/OELP/Models/3D Plot Datasets/" + modelname + "/"

  if os.path.exists(filepath):
    print ("Dataset folder exists")
    
  else:
    print ("Dataset folder does not exist")
    print ("Making Folder...")
    os.mkdir(filepath)

  if(save == 1):
    finalSampleSize = len(X) * int(totalSamples/sampleWindowSize)
    x = np.reshape(X,( finalSampleSize ,sampleWindowSize)) 

    train_data=np.zeros((finalSampleSize,sampleWindowSize,2))
    labels=np.zeros((finalSampleSize,len(CATEGORIES)))
    snr_labels = np.zeros((finalSampleSize,1))
    max_index = (len(X)/len(CATEGORIES) ) 

    for i in range(finalSampleSize):
      index = int(np.floor(i/ (finalSampleSize/len(CATEGORIES)) ))
      labels[i][index] = 1
      snr_labels[i] = snr_values[int((i/(finalSampleSize/len(X))))]
      
      for j in range(sampleWindowSize):
        train_data[i][j][0] = x[i][j].real
        train_data[i][j][1] = x[i][j].imag


    
    testSize = int(finalSampleSize * splitPercent)
    trainSize = int(finalSampleSize - testSize)

    X_train  = np.zeros((trainSize,sampleWindowSize,2))
    X_test = np.zeros((testSize,sampleWindowSize,2))
    y_train = np.zeros((trainSize,len(CATEGORIES)))
    y_test = np.zeros((testSize,len(CATEGORIES)))
    SNR_train = np.zeros((trainSize,1))
    SNR_test = np.zeros((testSize,1))

    trainModulationSize  = int(trainSize /len(CATEGORIES))
    testModulationSize  = int(testSize /len(CATEGORIES))
    trainSNRSize = int(trainModulationSize / len(SNR))
    testSNRSize = int(testModulationSize / len(SNR))
    FullModulationSize = int(finalSampleSize /len(CATEGORIES))
    FullSNRSize = int(FullModulationSize / len(SNR) )
    np.random.seed(5)

    from sklearn.utils import shuffle
    for i in range(len(CATEGORIES)):
      for j in range(len(SNR)):
        
        X_train[i*trainModulationSize + j*trainSNRSize : i*trainModulationSize + (j+1)*trainSNRSize ] ,X_test[i*testModulationSize + j*testSNRSize:i*testModulationSize + (j+1)*testSNRSize],y_train[i*trainModulationSize + j*trainSNRSize : i*trainModulationSize + (j+1)*trainSNRSize ] ,y_test[i*testModulationSize + j*testSNRSize:i*testModulationSize + (j+1)*testSNRSize], SNR_train[i*trainModulationSize + j*trainSNRSize : i*trainModulationSize + (j+1)*trainSNRSize ] ,SNR_test[i*testModulationSize + j*testSNRSize:i*testModulationSize + (j+1)*testSNRSize] = train_test_split(train_data[i*FullModulationSize + j*FullSNRSize:i*FullModulationSize + (j+1)*FullSNRSize],labels[i*FullModulationSize + j*FullSNRSize:i*FullModulationSize + (j+1)*FullSNRSize],snr_labels[i*FullModulationSize + j*FullSNRSize:i*FullModulationSize + (j+1)*FullSNRSize],test_size=0.1)  

    X_train,y_train,SNR_train = shuffle(X_train,y_train,SNR_train,random_state = 0) 
    print("Saving dataset...")
    X_train = np.expand_dims(X_train, -1)
    X_test = np.expand_dims(X_test, -1)
    np.save(filepath + 'X_train.npy', X_train)
    np.save(filepath + 'X_test.npy', X_test)
    np.save(filepath + 'y_train.npy', y_train)
    np.save(filepath + 'y_test.npy', y_test)
    np.save(filepath + 'SNR_train.npy', SNR_train)
    np.save(filepath + 'SNR_test.npy', SNR_test)
  elif(save == 0):
    print("Loading saved dataset....")
    X_train = np.load(filepath + 'X_train.npy')
    X_test = np.load(filepath +'X_test.npy')
    y_train = np.load(filepath +'y_train.npy')
    y_test = np.load(filepath +'y_test.npy')
    SNR_train = np.load(filepath + 'SNR_train.npy')
    SNR_test = np.load(filepath + 'SNR_test.npy')
    

  print("\nX_train : {}, y_train : {}".format(X_train.shape, y_train.shape))
  print("X_test : {}, y_test : {}".format(X_test.shape, y_test.shape))
  print("SNR_train : {}, SNR_test : {}".format(SNR_train.shape, SNR_test.shape))

  return (X_train,X_test,y_train,y_test,SNR_train,SNR_test)

"""## Function to Create Model"""

def createModel(X_train):

  filters = [(30,2),(15,1),(5,1)]
  if (X_train.shape[1] == 50):
    filters[0] = (5,2)
    filters[1] = (3,1)
    filters[2] = (3,1)
  
  filterNum = [32,64,128]
  if (X_train.shape[1] >= 1500):
    drop = 0.3
  else :
    drop = 0.2
  
  inputs = keras.Input(shape=X_train.shape[1:4], name='input')
  x = layers.Conv2D(filterNum[0], filters[0],strides=1,data_format='channels_last',padding='valid',name = 'conv1')(inputs)
  x = layers.BatchNormalization(axis = 3,name = 'bn_conv1')(x)
  x = layers.Activation('relu')(x)
  x = layers.AveragePooling2D((2,1),strides=2, name = 'avg_pool1')(x)
  x = layers.Dropout(drop)(x)

  x = layers.Conv2D(filterNum[1], filters[1],strides=1,padding='valid',name = 'conv2')(x)
  x = layers.BatchNormalization(axis = 3,name = 'bn_conv2')(x)
  x = layers.Activation('relu')(x)
  x = layers.AveragePooling2D((2,1),strides=2, name = 'Average_pool2')(x)
  x = layers.Dropout(drop)(x)

  x = layers.Conv2D(filterNum[2], filters[2],strides=1,padding='valid',name = 'conv3')(x)
  x = layers.BatchNormalization(axis = 3,name = 'bn_conv3')(x)
  x = layers.Activation('relu')(x)
  x = layers.AveragePooling2D((2,1),strides=2, name = 'Average_pool3')(x)
  x = layers.Dropout(drop)(x)

  x = layers.Flatten()(x)
  x = layers.Dense(256,activation='relu',name = 'Dense256')(x)
  outputs = layers.Dense(len(CATEGORIES),activation='softmax', name='output')(x)

  model = keras.Model(inputs=inputs, outputs=outputs,name = 'CNN{}'.format(X_train.shape[1]))
  model.summary()
  return model

"""## Function to Train Model"""

def trainModel(model,X_train,y_train,sampleWindowSize = 1000,load = 0):
  ACCURACY_THRESHOLD = 0.999
  class myCallback(tf.keras.callbacks.Callback): 
    
    def on_epoch_end(self, epoch, logs={}): 
          self.epoch = epoch
          if(logs.get('acc') > ACCURACY_THRESHOLD):   
            print("\nReached %2.2f%% accuracy, so stopping training!!" %(ACCURACY_THRESHOLD*100))   
            self.model.stop_training = True

  serial_no = sampleWindowSize
  modelname = 'CNN'+str(serial_no)
  modelpath = "/content/drive/My Drive/OELP/Models/3D Plot Models/" + modelname + "/"

  path = modelpath + "checkpoint"
  if os.path.exists(modelpath):
    print ("Model folder exists")
    
  else:
    print ("Model folder does not exist")
    print ("Making Folder...")
    os.mkdir(modelpath)
    
    
  callbacks = myCallback()
  stop_callbacks = keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0009, patience=1, verbose=1, mode='auto', baseline=None, restore_best_weights=True)
  
  checkpoint_path = modelpath  + modelname + ".ckpt"
  
  checkpoint_dir = os.path.dirname(checkpoint_path)

  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                  save_weights_only=True,
                                                 verbose=1)
  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy']) 

  if (os.path.exists(path) == 1 and (load == 0)):
    print ("Checkpoint at {} exists".format(checkpoint_path))
    print ("Continuing from checkpoint..")
    model.load_weights(checkpoint_path)

  try:
    history = model.fit(X_train, y_train,
                      batch_size=64,
                      epochs=20,
                      callbacks = [callbacks,cp_callback,stop_callbacks]  
    )  
    epoch = history.epoch
    print(epoch)  
  except Exception as e:
    print("\nGot \" {} \" exception".format(e))
    pass
  from contextlib import redirect_stdout
  filename = "CNN{}_summary.txt".format(sampleWindowSize)
  try:
    with open(os.path.join(modelpath,filename), 'w') as f:
        with redirect_stdout(f):
            model.summary()
            print("\nModel summary saved!")
  except Exception as e:
      print("Got",e, "exception")
      pass

  save_path = "/content/drive/My Drive/OELP/3D Plot Results/" 
  from contextlib import redirect_stdout
  filename = "CNN{}_Results.txt".format(sampleWindowSize)
  try:
    with open(os.path.join(save_path,filename), 'a') as f:
        with redirect_stdout(f):

            print("Train accuracy : {} \n".format(history.acc))
            print("Train loss : {} \n".format(history.loss))
            
    print("Training results saved!\n")
  except Exception as e:
      print("Got",e, "exception")
      pass
  model.save(modelpath + modelname + ".h5")   
  print("\n{}.h5 saved!".format(modelname))  

  return ( model, history )

"""## Function to Load Model"""

def loadModel(serial_no):

  modelname = 'CNN'+str(serial_no)
  modelpath = "/content/drive/My Drive/OELP/Models/3D Plot Models/" + modelname + "/"
  modelpath = modelpath  + modelname + ".h5"
  from tensorflow.keras.models import load_model
  model = load_model(modelpath,custom_objects=None,compile=True)
  return model

"""## Function to Evaluate Model"""

def save_obj(folderpath,obj, name ):
    with open(folderpath+ name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(folderpath,name ):
    with open(os.path.join(folderpath , name)  , 'rb') as f:
        return pickle.load(f)

def testModel(model,X_test,y_test,SNR_test,finalSampleSize = 3000,sampleWindowSize = 1000,splitPercent = 0.1,CATEGORIES = ["BPSK","16_PSK","4_QAM","16_QAM","8_PSK","64_QAM"]):
  results = model.evaluate(X_test, y_test)
  print('\nTest loss, Test acc:', results)

  from sklearn.metrics import classification_report, confusion_matrix
  
  plotdict = {}

  i =  0

  testSize = int(finalSampleSize * splitPercent)
  trainSize = int(finalSampleSize - testSize)

  trainModulationSize  = int(trainSize /len(CATEGORIES))
  testModulationSize  = int(testSize /len(CATEGORIES))
  trainSNRSize = int(trainModulationSize / len(SNR))
  testSNRSize = int(testModulationSize / len(SNR))
  FullModulationSize = int(finalSampleSize /len(CATEGORIES))
  FullSNRSize = int(FullModulationSize / len(SNR) )

  unique, counts = np.unique(SNR_test, return_counts=True)
  print("Count of unique SNR labels: ",dict(zip(unique, counts)))
  for category in CATEGORIES:
    plotdict[category] = []
    
    for j in range(len(SNR)):
      test_x = X_test[i*testModulationSize + j*testSNRSize : i*testModulationSize + (j+1)*testSNRSize] 
      test_y = y_test[i*testModulationSize + j*testSNRSize : i*testModulationSize + (j+1)*testSNRSize] 
      results = model.evaluate(test_x,test_y,verbose = 1)
      accuracy = results[1]
      loss = results[0]
      plotdict[category].append(accuracy) 
           
    i += 1
  
  Y_pred = model.predict(X_test)
  y_pred = np.argmax(Y_pred, axis=1)
  y_test1 = np.argmax(y_test, axis=1)
  print('\nConfusion Matrix')
  conf_mat = confusion_matrix(y_test1, y_pred)
  print(conf_mat)

  save_path = "/content/drive/My Drive/OELP/3D Plot Results/Confusion Matrix/" 
  filename = "CNN{}.npy".format(sampleWindowSize)

  if os.path.exists(save_path):
    print ("\nResult folder exists")
    
  else:
    print ("\nResult folder does not exist")
    print ("Making Folder...\n")
    os.mkdir(save_path)
  
  np.save(save_path + filename,conf_mat)
  print('\nClassification Report')

  print(classification_report(y_test1, y_pred, target_names=CATEGORIES))
  
  save_path = "/content/drive/My Drive/OELP/3D Plot Results/" 
  from contextlib import redirect_stdout
  filename = "CNN{}_Results.txt".format(sampleWindowSize)
  try:
    with open(os.path.join(save_path,filename), 'a') as f:
        with redirect_stdout(f):

            print("Test accuracy : {} \n".format(accuracy))
            print("Test loss : {} \n".format(loss))
            print(conf_mat)
            print("\n")
            print(classification_report(y_test1, y_pred, target_names=CATEGORIES))
            print("\n")
    print("Model results saved!")
  except Exception as e:
      print("Got",e, "exception")
      pass
  
  folderpath = "/content/drive/My Drive/OELP/3D Plot Results/Images/"
  if os.path.exists(folderpath):
    print ("\nImages folder exists")
    
  else:
    print ("\nImages folder does not exist")
    print ("Making Folder...\n")
    os.mkdir(folderpath)

  filename = "plot" +  str(sampleWindowSize)
  save_obj(folderpath,plotdict, filename) 
  return ( plotdict,filename )

def plotData(filename):
  folderpath = "/content/drive/My Drive/OELP/3D Plot Results/Images/"
  plotdict = load_obj(folderpath,filename)
  plt.figure(figsize=(15,15))
  x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
  plt.plot(x,plotdict["16_PSK"],marker = 'o')
  plt.plot(x,plotdict["16_QAM"],marker = '*')
  plt.plot(x,plotdict["4_QAM"],marker = '+')
  plt.plot(x,plotdict["8_PSK"],marker = 'x')
  plt.legend(("16_PSK","16_QAM","4_QAM","8_PSK"),loc = 'best')
  plt.xlabel("SNR(in dB)")
  plt.ylabel("Accuracy(in %)")
  plt.title(filename)

def trainAllModels(X,SNR,snr_values,totalSamples,windowSizes = [50, 100, 200, 500, 1000],exceptions = [200,500,1000],splitPercent = 0.1,save = 0, load = 1,CATEGORIES = ["BPSK","16_PSK","4_QAM","16_QAM","8_PSK","64_QAM"]) :
  for windowSize in windowSizes:

    if (windowSize in exceptions):
      save = 0
      load = 1
      print("Exception acknowledged.")
    print("Preparing data...")
    (X_train,X_test,y_train,y_test,SNR_train,SNR_test) = prepareData(X,SNR,snr_values,totalSamples = totalSamples,sampleWindowSize = windowSize,splitPercent=splitPercent,save=save,CATEGORIES=CATEGORIES )
    unique, counts = np.unique(SNR_test, return_counts=True)
    print(dict(zip(unique, counts)))
    print("Data prepared..")
    model = createModel(X_train)
    print("Model created.")
    
    if (load == 0):
      print("Training model...")
      model,history = trainModel(model,X_train,y_train,sampleWindowSize = windowSize,load = load)
    elif (load):
      print("Loading Model...")
      model = loadModel(serial_no = windowSize)
    
    save = 1
    load = 0
    print("Model trained and loaded...")
    finalSampleSize = len(SNR) * len(CATEGORIES) * int(totalSamples/windowSize)
    print("Evaluating model...")
    data, name = testModel(model,X_test,y_test,SNR_test,finalSampleSize=finalSampleSize,sampleWindowSize = windowSize, splitPercent=splitPercent,CATEGORIES=CATEGORIES )
  

def plotManyData():
  folderpath = "/content/drive/My Drive/OELP/3D Plot Results/Images/"
  savepath = "/content/drive/My Drive/OELP/3D Plot Results/Plots/"
  markers = ['s','o', '*', '+', 'x', 'd']
  for filename in os.listdir(folderpath):
    print(filename)
    plotdict = load_obj(folderpath,filename )
    fig = plt.figure(figsize=(15,15))
    ax = fig.add_subplot(111)
    x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
    for i in range(len(CATEGORIES)):
  
      try:
        ax.plot(x,plotdict[CATEGORIES[i]],marker = markers[i])     
        ax.set_xlabel("SNR(in dB)")
        ax.set_ylabel("Accuracy(in %)")
        ax.set_title("CNN" + filename[4:-4])
        ax.grid(b = True)
      except Exception as e:
        print("\nGot \"",e,"\" exception")
        pass
    ax.legend([CATEGORIES[i] for i in range(len(CATEGORIES))],loc = 'best')
    for i,j in zip(x,plotdict[CATEGORIES[0]]):
      ax.annotate(str(j),xy=(i,j), xytext = (15,10), textcoords = 'offset points')
    for i,j in zip(x,plotdict[CATEGORIES[1]]):
      ax.annotate(str(j),xy=(i,j), xytext = (15,10), textcoords = 'offset points')
    for i,j in zip(x,plotdict[CATEGORIES[2]]):
      ax.annotate(str(j),xy=(i,j), xytext = (-10,-10), textcoords = 'offset points')
    for i,j in zip(x,plotdict[CATEGORIES[4]]):
      ax.annotate(str(j),xy=(i,j), xytext = (-15,10), textcoords = 'offset points')
    
    if not os.path.exists(savepath):
      os.mkdir(savepath)
    plt.savefig(os.path.join(savepath,filename[0:-4]))

"""## Set Up"""

import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat,savemat
import os
import pandas as pd
import pickle  
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers

DataDir = "/content/drive/My Drive/OELP/Dataset 09-03-20/"
CATEGORIES = ["BPSK","16_PSK","4_QAM","16_QAM","8_PSK","64_QAM"]
SNR = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
!pip show tensorflow

"""## Execution

## Testing trainAllModels
"""

X,snr_values = loadData()
totalSamples = X.shape[-1]
print(totalSamples)

trainAllModels(X,SNR,snr_values,totalSamples,windowSizes = [2000,5000], exceptions = [5,10,50,100,200,500,1000],splitPercent = 0.1, save = 0,load = 0,CATEGORIES=CATEGORIES)

"""## Testing plotManyData"""

plotManyData()

"""#3D Plot of Accuracy Vs SNR Vs No of samples"""

def loadPlotdict(modelfolder,name):
  folderpath = ("/content/drive/My Drive/OELP/3D Plot Results/{}/Images/").format(modelfolder)
  plotdict = load_obj(folderpath,name)
  return plotdict

def Plot(X,Y,Z):

  offset = 0
  filenames = ['plot50.pkl','plot100.pkl','plot200.pkl','plot500.pkl','plot1000.pkl']
  for j in range(len(CATEGORIES)):

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    categoryValues_X = []
    categoryValues_Y = []
    categoryValues_Z = []
    for i in range(len(filenames)):
      categoryValues_X.append(X[i*90 + offset :i*90 + offset  + 15])
      categoryValues_Y.append(Y[i*90 + offset :i*90 + offset  + 15])
      categoryValues_Z.append(Z[i*90 + offset :i*90 + offset  + 15])
    ax.scatter(categoryValues_X,categoryValues_Y,categoryValues_Z, c='b', marker='o')

    ax.set_xlabel('X Label')
    ax.set_ylabel('Y Label')
    ax.set_zlabel('Z Label')
    ax.title(CATEGORIES[j])

    offset += 15

  plt.show()

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
CATEGORIES = ["BPSK","16_PSK","4_QAM","16_QAM","8_PSK","64_QAM"]
SNR = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]


X = []
Y = []
Z = []

modelfolder = "Evaluation Model Result"
folderpath = ("/content/drive/My Drive/OELP/3D Plot Results/{}/Images/").format(modelfolder)

filenames = ['plot50.pkl','plot100.pkl','plot200.pkl','plot500.pkl','plot1000.pkl']
for filename in filenames:
  try:
    plotdict = loadPlotdict(modelfolder,filename)

    for category in CATEGORIES:

      for i in range(len(SNR)):
        X.append(SNR[i])
        Y.append(int(filename[4:-4]))
      for val in plotdict[category]:
        Z.append(val)

    

  except Exception as e:
    print("Got \"{}\" exception".format(e))
  

Plot(X,Y,Z)

